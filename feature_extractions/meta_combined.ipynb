{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3005ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc73125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the metadata. We'll only import the columns we need.\n",
    "if not os.path.exists(\"../Data/metadata_raw.pkl\"): #Converts json to pkl if needed\n",
    "    print(\"Converting json file to pkl\")\n",
    "    metadata_df_raw = pd.read_json(\"../Data/amazon_meta.json\",lines=True)\n",
    "    metadata_df_raw.to_pickle('../Data/metadata_raw.pkl')\n",
    "    del metadata_df_raw\n",
    "columns=['asin','category', 'title', 'price','also_buy', 'also_view']\n",
    "combined_df=pd.read_pickle(\"../Data/metadata_raw.pkl\")[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97f0e5",
   "metadata": {},
   "source": [
    "##### Cleaning Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061efb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove price columns starting .a-box-inner\n",
    "\n",
    "def filter_junk(price):\n",
    "    if price=='':\n",
    "        return None\n",
    "    if len(price)>=12:\n",
    "        if price[0:12]=='.a-box-inner':\n",
    "            return None\n",
    "    return price\n",
    "\n",
    "combined_df.price=combined_df.price.apply(filter_junk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c287231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing_price feature\n",
    "# Might indicate item is no longer for sale/out of stock?\n",
    "combined_df['missing_price']=combined_df.price.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21523b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove price\n",
    "combined_df=combined_df.drop(columns='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3688248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633883/633883 [00:00<00:00, 747588.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replace category with top-level subcategory\n",
    "def extract_subcategory(cat):\n",
    "    if len(cat)>0:\n",
    "        return cat[1]\n",
    "    return None\n",
    "\n",
    "combined_df['category']=combined_df.category.progress_apply(extract_subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4575fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/633883 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633883/633883 [00:27<00:00, 23294.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combine also_buy and also_view into a single list\n",
    "def combine(entry):\n",
    "    output=entry.also_buy+entry.also_view\n",
    "    output=list(set(output)) # Remove duplicates\n",
    "    if output==[]:\n",
    "        return None\n",
    "    return output\n",
    "    \n",
    "combined_df['similar']=combined_df.progress_apply(combine,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2442f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now fill in missing categories\n",
    "# We create a lookup table of all items that already have a category.\n",
    "lookup_cat=dict()\n",
    "for index,entry in combined_df[combined_df.category.notna()].iterrows():\n",
    "    lookup_cat[entry.asin]=entry.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249a10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function predicts the category using the category of similar (also_view or also_buy) products.\n",
    "# We pick the most frequently ocurring such category\n",
    "from collections import Counter\n",
    "\n",
    "def most_frequent(lst):\n",
    "    return Counter(lst).most_common(1)[0][0]\n",
    "\n",
    "def predict_category_similar_prods(entry,i=0):\n",
    "    if entry.category!=None:\n",
    "        return entry.category\n",
    "    similar_prods=entry.similar\n",
    "    if similar_prods==None:\n",
    "        return None\n",
    "    similar_categories=[]\n",
    "    for prod in similar_prods:\n",
    "        category=lookup_cat.get(prod,-1)\n",
    "        if category!=-1:\n",
    "            similar_categories.append(category)\n",
    "    if len(similar_categories)>0:\n",
    "        return most_frequent(similar_categories)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f1fc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in as many missing category values as possible\n",
    "combined_df.category=combined_df.apply(predict_category_similar_prods,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de73fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-hashable columns that are no longer needed\n",
    "combined_df=combined_df.drop(columns=['also_buy','also_view','similar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757055d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "combined_df=combined_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24db049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify asins are unique\n",
    "assert(combined_df.asin.duplicated().unique()==[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e060c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ranks.\n",
    "ranks_df=pd.read_parquet(\"../Data/meta_ranks.parquet\")\n",
    "ranks_df=ranks_df.drop_duplicates()\n",
    "\n",
    "# Verify no duplicated asins\n",
    "assert(ranks_df.asin.duplicated().unique()==[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9653caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ranks with combined_df\n",
    "combined_df=combined_df.merge(ranks_df,on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c57dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing items with missing rank or missing category\n",
    "pd.DataFrame.dropna(combined_df,axis=0,subset=['category','item_rank'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb3858",
   "metadata": {},
   "source": [
    "##### Add matches and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48f67e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matches with components\n",
    "matches_df=pd.read_csv(\"../Data/amazon_df_labels_with_comps.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e94b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge matches with combined_df\n",
    "combined_df=combined_df.merge(matches_df,on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d81994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import undropped asins after cleaning review_data.\n",
    "review_cleaned_asins=pd.read_csv(\"../Data/asin_labels_clean_review_df.csv\")\n",
    "\n",
    "# Check no duplicate asin\n",
    "assert(review_cleaned_asins.asin.duplicated().unique()==[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf60bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entries from combined_df that don't appear in review_cleaned_asins\n",
    "combined_df=combined_df.merge(review_cleaned_asins[['asin']],on='asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801f5d0",
   "metadata": {},
   "source": [
    "##### Add reviews features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb1dc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reviews_df (from reviews_features.ipynb)\n",
    "reviews_features_df=pd.read_parquet(\"final_reviews.parquet\")\n",
    "\n",
    "# Verify no duplicated asins\n",
    "assert(reviews_features_df.asin.duplicated().unique()==[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de447c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge meta and review datasets\n",
    "combined_df=combined_df.merge(reviews_features_df,on='asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6324d8",
   "metadata": {},
   "source": [
    "##### Add embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb884709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings=pd.read_pickle(\"../Data/agg_summary_embeddings.pkl\")\n",
    "\n",
    "# Verify no duplicated asins\n",
    "assert(embeddings.asin.duplicated().unique()==[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67f9db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df.merge(embeddings,on='asin',how='left').set_axis(combined_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf8138",
   "metadata": {},
   "source": [
    "##### Randomly dropping items labelled zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc97c5a",
   "metadata": {},
   "source": [
    "To obtain a more manageable data set, we randomly drop items whose ``match`` column is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e0f99c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 546348 products labelled 0.\n",
      "There are 1370 products labelled 1.\n"
     ]
    }
   ],
   "source": [
    "num_zeros=(combined_df.match==0).sum()\n",
    "num_ones=(combined_df.match==1).sum()\n",
    "print(f\"There are {num_zeros} products labelled 0.\")\n",
    "print(f\"There are {num_ones} products labelled 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f345e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of 0s in our final dataset.\n",
    "TARGET=200_000\n",
    "\n",
    "# Check TARGET is less than the total number of zeros\n",
    "assert(TARGET<num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2677994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of items labelled zero\n",
    "zero_indices=combined_df[combined_df.match==0].index\n",
    "# indices of items labelled one\n",
    "one_indices=combined_df[combined_df.match==1].index\n",
    "\n",
    "# random subset of TARGET items labelled zero\n",
    "rng=np.random.default_rng(seed=1067)\n",
    "random_subset=rng.choice(zero_indices,TARGET,replace=False)\n",
    "\n",
    "# total list of indices\n",
    "indices=np.concat([random_subset,one_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdbb97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows of combined_df with index not in indices\n",
    "combined_df=combined_df.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "521816eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n",
      "0    200000\n",
      "1      1370\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.match.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca00676",
   "metadata": {},
   "source": [
    "##### Stratified train-test split preserving groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25159581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_ttsplit import StratifiedGroupSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac6fe1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test=StratifiedGroupSplit(combined_df,'match','component_no',test_size=0.2,random_state=1066)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68b31f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check test ratio\n",
    "df_test.shape[0]/combined_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffee2c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068033967323831756"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check ratio of 1s in set before split\n",
    "combined_df[(combined_df.match)==1].shape[0]/combined_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3d6878c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068033967323831756"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check ratio of 1s in test set\n",
    "df_test[(df_test.match)==1].shape[0]/df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "116c850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068033967323831756"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check ratio of 1s in training set\n",
    "df_train[(df_train.match)==1].shape[0]/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae165d1",
   "metadata": {},
   "source": [
    "These numbers are all very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "338c0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no component overlaps\n",
    "comps_in_test=set(df_test.component_no.unique())\n",
    "comps_in_train=set(df_train.component_no.unique())\n",
    "assert(comps_in_train.intersection(comps_in_test)==set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a877fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change these files unless the train-test split is changed\n",
    "# df_train[['asin']].to_parquet(\"../Data/train_asins.parquet\",compression='gzip')\n",
    "# df_test[['asin']].to_parquet(\"../Data/test_asins.parquet\",compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffb24de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we haven't changed the train-test split asins from the split on 13 Jun\n",
    "saved_train=pd.read_parquet(\"../Data/train_asins.parquet\")\n",
    "saved_test=pd.read_parquet(\"../Data/test_asins.parquet\")\n",
    "assert(saved_train.shape[0]==df_train.shape[0])\n",
    "assert((saved_train.asin!=df_train.asin).sum()==0)\n",
    "assert(saved_test.shape[0]==df_test.shape[0])\n",
    "assert((saved_test.asin!=df_test.asin).sum()==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285630c",
   "metadata": {},
   "source": [
    "##### Save to compressed parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(\"../Data/train_v1.parquet\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445aafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(\"../Data/test_v1.parquet\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save md5sums of files\n",
    "# import hashlib\n",
    "\n",
    "# def calculate_md5(filepath):\n",
    "#     md5_hash = hashlib.md5()\n",
    "#     with open(filepath, \"rb\") as file:\n",
    "#         # Read the file in chunks to handle large files\n",
    "#         for chunk in iter(lambda: file.read(4096), b\"\"):\n",
    "#             md5_hash.update(chunk)\n",
    "#     return md5_hash.hexdigest()\n",
    "\n",
    "# os.chdir(\"../Data/\")\n",
    "# file_list=[\"df_train_v1.parquet\",\"df_test_v1.parquet\"]\n",
    "# output_file = \"../Data/md5_checksums.txt\"\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for file_path in file_list:\n",
    "#         md5_value = calculate_md5(file_path)\n",
    "#         f.write(f\"{md5_value}  {file_path}\\n\")\n",
    "# os.chdir(\"../feature_extractions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e8a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

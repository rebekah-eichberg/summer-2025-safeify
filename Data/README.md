# Data Folder README

This `Data` folder contains all datasets and related files used for the Safeify project’s exploratory data analysis (EDA), cleaning, feature engineering, and modeling. Below is a description of each file and subfolder currently present.


## File List & Descriptions

- **asin_labels_clean_review_df.csv**  
  CSV file with cleaned Amazon products and their matchings (labels) (size: 10M).

- **duckduckgo_search_results.csv**  
  CSV file of DuckDuckGo scraped search results (size: 4.5M).

- **cpsc_data/**  
  Directory containing raw CPSC (Consumer Product Safety Commission) recall and incident report data.

- **md5_checksums.txt**  
  Text file with MD5 checksums for data integrity verification (size: 276 bytes).

The following data files are not included included in the github repository, but can be downloaded using `.py` scripts.

#### To download the following files, run `../src/download_split_finaldata.py`

- **test_v3.parquet**  
  Parquet file for the test split of the dataset (size: 201M).

- **train_final_v3.parquet**  
  Parquet file for the final training split (size: 577M).

- **validationA_v3.parquet**  
  Parquet file for validation split (size: 121M).

- **validationB_v3.parquet**  
  Parquet file for calibration split (size: 121M).

- **CV_val_split.parquet**  
  Parquet file with indices need for cross-validation validation split (size: 802K).

#### To download the following files, run `../src/download_embeddings.py`

- **agg_summary_embeddings.pkl**  
  Pickled file containing aggregated summary embeddings for reviews or products (size: 1.8G).

- **reviewtext_features_df.pkl**  
  Pickled DataFrame with text embeddings from review text (size: 947M).

#### To download the following files, run `../src/download_amazon_data.py`

- **amazon_meta.json**  
  Raw Amazon product metadata in JSON format (size: 7.2G).

- **amazon_reviews.json**  
  Raw Amazon product reviews in JSON format (size: 3.4G).

- **metadata_raw.pkl**  
  Raw product metadata in pickle format (size: 6.9G).

- **reviews_raw.pkl**  
  Raw product reviews DataFrame in pickle format (size: 2.6G).

In addition, there are several intermediate files generated by notebooks and py scripts in this repository.

## Usage

1. The raw amazon.com product metadata and reviews data can be downloaded by running `../src/download_amazon_data.py`. **This is only needed to run notebooks in the `matching` and `cleaning_and_feature_engineering` folders, not for modeling.**

2. The aggregated text embeddings for review and summary text can be downloaded by running `../src/download_amazon_data.py`. These files are generated by notebooks in the `cleaning_and_feature_engineering` folder, but since they took a while to generate, we include a script to download them. **This is only needed to run notebooks in the `cleaning_and_feature_engineering` folders, not for modeling.**

3. The final dataset used for modeling can be downloaded by running `../src/download_split_finaldata.py`. It is split into a training set, a validation set, a calibration set and a testing set, as described in [this readme](../cleaning_and_feature_engineering/README.md). It is generated by the notebook `cleaning_and_feature_engineering/combining_and_splitting.ipynb`. **This is needed for the final modeling step.**

## Description of Columns in the Final Dataset

- **category**  
  Second-level Amazon product category (e.g., "Puzzles" from "Toys & Games > Puzzles > Jigsaw Puzzles").

- **missing_price**  
  Indicates if the product's price was missing in the metadata (may signal out-of-stock or unavailable items).

- **item_rank**  
  Amazon Best Sellers rank for the product with the `Toys and Games` category.

- **match**  
  1 if the product matched a CPSC incident/recall, 0 otherwise. **This is our target variable.**

- **avg_rating**  
  Average rating for the product.

- **min_rating**  
  Minimum rating received by the product.

- **num_of_rating**  
  Total number of ratings for the product.

- **percent_positive**  
  Percentage of positive ratings (ratings ≥ 4).

- **percent_negative**  
  Percentage of negative ratings (ratings ≤ 2).

- **avg_verified_reviewers**  
  Percentage of reviews from verified purchasers.

- **min_date**  
  Date of the earliest review for the product.

- **max_date**  
  Date of the most recent review for the product.

- **product_lifespan**  
  Time span between the first and last reviews.

- **num_bots_per_asin**  
  Number of suspected bot reviewers for the product. See [this readme](../cleaning_and_feature_engineering/README.md) for the criterion for when a reviewer is a suspected bot.

- **unique_reviewer_count**  
  Number of unique reviewers for the product.

- **avg_reviews_per_day**  
  Average number of reviews per day over the product's lifespan.

- **reviews_per_product**  
  Total number of reviews for the product.

- **avg_review_length_words**  
  Average review length in words.

- **embed_0, embed_1, ..., embed_N**  
  Components of the text embedding vector representing the product summary text (aggregated by product).

- **embedding_0, embedding_1, ..., embedding_M**  
  Components of the text embedding vector representing the product review text (aggregated by product).

- **mean_sentiment_score**  
  The average sentiment score of all reviews for the product (higher means more positive sentiment).

- **mean_complaint_similarity**  
  The average similarity between each review and a collection of reference complaint sentences (higher means reviews are more complaint-like).

- **mean_shipping_similarity**  
  The average similarity between each review and shipping-related reference complain sentences (higher means reviews mention shipping issues more often).

- **max_complaint_similarity**  
  The highest complaint similarity score among all reviews for the product (captures the most complaint-like review).

- **shipping_similarity_at_max_complaint**  
  The shipping similarity score for the review that had the maximum complaint similarity (shows if the most complaint-like review also mentioned shipping).

- **sentiment_score_at_max_complaint**  
  The sentiment score for the review with the maximum complaint similarity (shows if the most complaint-like review was positive or negative).


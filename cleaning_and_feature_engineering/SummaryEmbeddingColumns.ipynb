{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0776d90f",
   "metadata": {},
   "source": [
    "# Documentation for `SummaryEmbeddingColumns.ipynb`\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook processes Amazon product review summaries to generate sentiment-weighted sentence embeddings for each product. \n",
    "\n",
    "## Main Steps\n",
    "\n",
    "We use the `all-MiniLM-L6-v2` SentenceTransformer model to embed each review summary into a 384-dimensional vector. We then use the `cardiffnlp/twitter-roberta-base-sentiment` model to compute sentiment scores for each summary. The negative sentiment probability is used as a weight for each summary.\n",
    "\n",
    "For each ASIN, we then compute a sentiment-weighted average of all summary embeddings, resulting in one embedding vector per product.\n",
    "\n",
    "## Input Files\n",
    "\n",
    "- `../Data/amazon_reviews.json` — Raw Amazon review data.\n",
    "- `../Data/asin_labels_clean_review_df.csv` — Cleaned ASIN labels.\n",
    "- `../Data/cpsc_data/incident_reports/*.csv` — CPSC incident reports (for reference).\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- `../Data/agg_summary_embeddings.pkl` — Pickle file containing the final DataFrame with ASINs and their 384-dimensional summary embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73411066-7806-4359-810e-5c951f356619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Betul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')       \n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt_tab')  \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94d952-9a74-4ea7-9fd1-ad08997138a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/amazon_reviews.json'  \n",
    "reviews_df = pd.read_json(file_path, lines=True, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81841ff1-a353-45b6-83ed-93523c2aa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_csv(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the first line that contains all expected headers (starts with 'Report No.')\n",
    "    header_index = next(i for i, line in enumerate(lines) if 'Report No.' in line)\n",
    "\n",
    "    # Load CSV from that line forward\n",
    "    return pd.read_csv(path, skiprows=header_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31fcd7-f877-4345-b4e5-3be908957711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to all three files\n",
    "recall_files = [\n",
    "    \"../Data/cpsc_data/incident_reports/Toysandchildren_ArtsandCrafts.csv\",\n",
    "    \"../Data/cpsc_data/incident_reports/Toysandchildren_Riding_Toys.csv\",\n",
    "    \"../Data/cpsc_data/incident_reports/Toysandchildren_Toys.csv\"\n",
    "]\n",
    "\n",
    "recall_dfs = [load_clean_csv(path) for path in recall_files]\n",
    "recalls_df = pd.concat(recall_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf69bd79-4e3e-4b62-8d09-a6cf7fa26e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ee725b-1ad5-4fac-89b1-3ff2fc1be445",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df[['asin', 'reviewText', 'summary' ,'overall']].copy()\n",
    "reviews_df = reviews_df.dropna(subset=['asin','reviewText', 'summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc90bf-85f8-4b70-abac-18cd1bdefc5d",
   "metadata": {},
   "source": [
    "### Upload Cleaned Asins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d8f70-842e-4e1b-82bd-b34c206b9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_labels_clean_review_df = pd.read_csv('../Data/asin_labels_clean_review_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395a196e-4189-4ac7-b71f-6c6bfca6e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614658, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asin_labels_clean_review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad940cdc-6854-49a3-a462-30188b9dad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to get asins in clean asins\n",
    "reviews_df = reviews_df[reviews_df['asin'].isin(asin_labels_clean_review_df['asin'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38c98a1-ed13-43ca-8aa0-2744a02bb23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8172849, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6156fec-6968-4153-baa4-c9ee88d2d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip possible leading or trailing white space\n",
    "reviews_model_df = reviews_df[reviews_df['summary'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d8630d6-2eb6-4420-97c8-c98b9204b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialize various packages to create embeddings on summary text\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2f17d48-5dfa-488a-b301-b7a27ff231b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985c273a67054d6fbcc30df81f75d770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/255401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embed the summary with sentence transformers\n",
    "summary_embeddings = model.encode(\n",
    "    reviews_model_df['summary'].tolist(),\n",
    "    batch_size=32,              \n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acb5ddc0-1513-4967-8f06-7654d7bcba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\AppData\\Local\\Temp\\ipykernel_18240\\252148832.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_model_df['summary_embeddings'] = list(summary_embeddings)\n"
     ]
    }
   ],
   "source": [
    "reviews_model_df['summary_embeddings'] = list(summary_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6788252-5c6a-4f09-9586-d8646b17e3dc",
   "metadata": {},
   "source": [
    "## Compute Sentiment of Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdaefdee-66fa-4a0c-aacd-9667df0cec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_sent = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_sent = model_sent.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfdac2-fe99-4f6d-acbb-ae255273dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def batched_sentiment_weights(texts, batch_size=64):\n",
    "    sentiment_scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model_sent(**inputs).logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy() \n",
    "        sentiment_scores.extend(probs[:, 0])  \n",
    "\n",
    "    return sentiment_scores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97888edf-60e5-4ff9-a1d6-5d9a4148f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batched_sentiment_weights(texts, batch_size=64):\n",
    "    sentiment_scores = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_sent.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize and move inputs to the correct device\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            logits = model_sent(**inputs).logits\n",
    "\n",
    "        # Get probability of negative sentiment (index 0)\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()\n",
    "        sentiment_scores.extend(probs[:, 0])  # or change index for different sentiment\n",
    "\n",
    "    return sentiment_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b21ae5cb-69c7-49ec-8bb4-01c9c2d0cc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/127701 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 127701/127701 [48:52<00:00, 43.55it/s]\n",
      "C:\\Users\\Betul\\AppData\\Local\\Temp\\ipykernel_18240\\1820920423.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_model_df['sentiment_weight'] = batched_sentiment_weights(reviews_model_df['summary'].tolist())\n"
     ]
    }
   ],
   "source": [
    "reviews_model_df['sentiment_weight'] = batched_sentiment_weights(reviews_model_df['summary'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af339935-95c4-42d2-acff-7a6d14ea41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_embedding(group):\n",
    "    weights = np.array(group['sentiment_weight'].tolist())\n",
    "    embeddings = np.stack(group['summary_embeddings'].tolist())\n",
    "    if weights.sum() == 0:\n",
    "        weights = np.ones_like(weights)\n",
    "    return np.average(embeddings, axis=0, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fe06a00-70cf-4cac-9efb-a49c3b5745af",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_summary_embeddings = reviews_model_df.groupby('asin', group_keys=False).apply(\n",
    "    weighted_avg_embedding, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c51b82a7-9971-4e4f-870d-b73000b048bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin\n",
       "0000191639    [-0.023288769647479057, 0.014182460494339466, ...\n",
       "0004950763    [-0.08644748479127884, -0.009291399270296097, ...\n",
       "0004983289    [-0.0328253600229539, 0.013325432199417255, -0...\n",
       "0005069491    [-0.06845328211784363, -0.06251281499862671, 0...\n",
       "0020232233    [-0.033952606099107614, 0.035724348085384536, ...\n",
       "                                    ...                        \n",
       "B01HJDFWDK    [-0.051193756597999227, -0.0038128394320709443...\n",
       "B01HJDGVFS    [-0.05726094457197995, -0.009238787646112264, ...\n",
       "B01HJDUNRU    [-0.030438451488812853, 0.022333641545807352, ...\n",
       "B01HJFAGJI    [-0.007043351280748524, 0.03934371761938393, -...\n",
       "B01HJHA7GI    [-0.09151761642866257, 0.00693787157468007, 0....\n",
       "Length: 614657, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_summary_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a6e05b3-2a12-4249-8c21-21145214a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614657,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_summary_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09301896-54b4-436f-a56c-7a8046dbb447",
   "metadata": {},
   "source": [
    "### Make 384 Columns for The Summary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1695e0c6-93eb-404f-91d3-164eca88de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.DataFrame(agg_summary_embeddings.tolist(), index=agg_summary_embeddings.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d35bd85a-714e-435e-b4da-92c74a003d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000191639</th>\n",
       "      <td>-0.023289</td>\n",
       "      <td>0.014182</td>\n",
       "      <td>-0.045827</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>-0.059605</td>\n",
       "      <td>0.021759</td>\n",
       "      <td>0.062154</td>\n",
       "      <td>-0.047039</td>\n",
       "      <td>-0.048791</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076976</td>\n",
       "      <td>-0.025515</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.026689</td>\n",
       "      <td>-0.033182</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>0.079272</td>\n",
       "      <td>0.126950</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>0.022833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004950763</th>\n",
       "      <td>-0.086447</td>\n",
       "      <td>-0.009291</td>\n",
       "      <td>0.025128</td>\n",
       "      <td>0.071182</td>\n",
       "      <td>-0.010236</td>\n",
       "      <td>0.034107</td>\n",
       "      <td>0.092472</td>\n",
       "      <td>-0.090956</td>\n",
       "      <td>0.009852</td>\n",
       "      <td>0.018048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>-0.018429</td>\n",
       "      <td>-0.019864</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.087291</td>\n",
       "      <td>-0.032195</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.052647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004983289</th>\n",
       "      <td>-0.032825</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>-0.046219</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.079885</td>\n",
       "      <td>-0.070505</td>\n",
       "      <td>0.024829</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>-0.008946</td>\n",
       "      <td>-0.023322</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.010961</td>\n",
       "      <td>0.017847</td>\n",
       "      <td>0.098809</td>\n",
       "      <td>-0.034606</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>0.051882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005069491</th>\n",
       "      <td>-0.068453</td>\n",
       "      <td>-0.062513</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>0.045730</td>\n",
       "      <td>-0.081839</td>\n",
       "      <td>-0.045375</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>-0.069091</td>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.003694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059335</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>-0.004571</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>-0.053182</td>\n",
       "      <td>0.051308</td>\n",
       "      <td>0.046214</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>-0.108413</td>\n",
       "      <td>-0.007681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0020232233</th>\n",
       "      <td>-0.033953</td>\n",
       "      <td>0.035724</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.082713</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.021002</td>\n",
       "      <td>-0.018459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>-0.031809</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>-0.063010</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.030218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5    \\\n",
       "asin                                                                     \n",
       "0000191639 -0.023289  0.014182 -0.045827  0.004536 -0.059605  0.021759   \n",
       "0004950763 -0.086447 -0.009291  0.025128  0.071182 -0.010236  0.034107   \n",
       "0004983289 -0.032825  0.013325 -0.000896  0.053275 -0.046219  0.025191   \n",
       "0005069491 -0.068453 -0.062513  0.055283  0.045730 -0.081839 -0.045375   \n",
       "0020232233 -0.033953  0.035724  0.000368  0.000224  0.005743  0.013830   \n",
       "\n",
       "                 6         7         8         9    ...       374       375  \\\n",
       "asin                                                ...                       \n",
       "0000191639  0.062154 -0.047039 -0.048791  0.025406  ...  0.076976 -0.025515   \n",
       "0004950763  0.092472 -0.090956  0.009852  0.018048  ...  0.004744 -0.018429   \n",
       "0004983289  0.079885 -0.070505  0.024829  0.002191  ...  0.010980 -0.008946   \n",
       "0005069491  0.076266 -0.069091 -0.059432 -0.003694  ... -0.059335  0.031384   \n",
       "0020232233  0.082713  0.004960  0.021002 -0.018459  ...  0.022737 -0.028082   \n",
       "\n",
       "                 376       377       378       379       380       381  \\\n",
       "asin                                                                     \n",
       "0000191639  0.004724  0.026689 -0.033182  0.019103  0.079272  0.126950   \n",
       "0004950763 -0.019864 -0.008816  0.020599  0.028047  0.087291 -0.032195   \n",
       "0004983289 -0.023322  0.011250  0.010961  0.017847  0.098809 -0.034606   \n",
       "0005069491 -0.004571  0.078000 -0.053182  0.051308  0.046214  0.022991   \n",
       "0020232233  0.023483  0.006377 -0.031809  0.014866  0.051410 -0.063010   \n",
       "\n",
       "                 382       383  \n",
       "asin                            \n",
       "0000191639  0.030770  0.022833  \n",
       "0004950763  0.001828  0.052647  \n",
       "0004983289 -0.001182  0.051882  \n",
       "0005069491 -0.108413 -0.007681  \n",
       "0020232233  0.000670  0.030218  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bebb39ad-2bae-427a-b6d4-ae510844531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.columns = [f's_embed_{i}' for i in range(384)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6237c3a5-dc47-4821-8557-0b35daaf6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = expanded_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79e7b68d-1cf1-420e-ae63-286b295f2e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>s_embed_0</th>\n",
       "      <th>s_embed_1</th>\n",
       "      <th>s_embed_2</th>\n",
       "      <th>s_embed_3</th>\n",
       "      <th>s_embed_4</th>\n",
       "      <th>s_embed_5</th>\n",
       "      <th>s_embed_6</th>\n",
       "      <th>s_embed_7</th>\n",
       "      <th>s_embed_8</th>\n",
       "      <th>...</th>\n",
       "      <th>s_embed_374</th>\n",
       "      <th>s_embed_375</th>\n",
       "      <th>s_embed_376</th>\n",
       "      <th>s_embed_377</th>\n",
       "      <th>s_embed_378</th>\n",
       "      <th>s_embed_379</th>\n",
       "      <th>s_embed_380</th>\n",
       "      <th>s_embed_381</th>\n",
       "      <th>s_embed_382</th>\n",
       "      <th>s_embed_383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000191639</td>\n",
       "      <td>-0.023289</td>\n",
       "      <td>0.014182</td>\n",
       "      <td>-0.045827</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>-0.059605</td>\n",
       "      <td>0.021759</td>\n",
       "      <td>0.062154</td>\n",
       "      <td>-0.047039</td>\n",
       "      <td>-0.048791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076976</td>\n",
       "      <td>-0.025515</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.026689</td>\n",
       "      <td>-0.033182</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>0.079272</td>\n",
       "      <td>0.126950</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>0.022833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0004950763</td>\n",
       "      <td>-0.086447</td>\n",
       "      <td>-0.009291</td>\n",
       "      <td>0.025128</td>\n",
       "      <td>0.071182</td>\n",
       "      <td>-0.010236</td>\n",
       "      <td>0.034107</td>\n",
       "      <td>0.092472</td>\n",
       "      <td>-0.090956</td>\n",
       "      <td>0.009852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>-0.018429</td>\n",
       "      <td>-0.019864</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.087291</td>\n",
       "      <td>-0.032195</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.052647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004983289</td>\n",
       "      <td>-0.032825</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>-0.046219</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.079885</td>\n",
       "      <td>-0.070505</td>\n",
       "      <td>0.024829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>-0.008946</td>\n",
       "      <td>-0.023322</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.010961</td>\n",
       "      <td>0.017847</td>\n",
       "      <td>0.098809</td>\n",
       "      <td>-0.034606</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>0.051882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005069491</td>\n",
       "      <td>-0.068453</td>\n",
       "      <td>-0.062513</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>0.045730</td>\n",
       "      <td>-0.081839</td>\n",
       "      <td>-0.045375</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>-0.069091</td>\n",
       "      <td>-0.059432</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059335</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>-0.004571</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>-0.053182</td>\n",
       "      <td>0.051308</td>\n",
       "      <td>0.046214</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>-0.108413</td>\n",
       "      <td>-0.007681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0020232233</td>\n",
       "      <td>-0.033953</td>\n",
       "      <td>0.035724</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.082713</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.021002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>-0.031809</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>-0.063010</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.030218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  s_embed_0  s_embed_1  s_embed_2  s_embed_3  s_embed_4  \\\n",
       "0  0000191639  -0.023289   0.014182  -0.045827   0.004536  -0.059605   \n",
       "1  0004950763  -0.086447  -0.009291   0.025128   0.071182  -0.010236   \n",
       "2  0004983289  -0.032825   0.013325  -0.000896   0.053275  -0.046219   \n",
       "3  0005069491  -0.068453  -0.062513   0.055283   0.045730  -0.081839   \n",
       "4  0020232233  -0.033953   0.035724   0.000368   0.000224   0.005743   \n",
       "\n",
       "   s_embed_5  s_embed_6  s_embed_7  s_embed_8  ...  s_embed_374  s_embed_375  \\\n",
       "0   0.021759   0.062154  -0.047039  -0.048791  ...     0.076976    -0.025515   \n",
       "1   0.034107   0.092472  -0.090956   0.009852  ...     0.004744    -0.018429   \n",
       "2   0.025191   0.079885  -0.070505   0.024829  ...     0.010980    -0.008946   \n",
       "3  -0.045375   0.076266  -0.069091  -0.059432  ...    -0.059335     0.031384   \n",
       "4   0.013830   0.082713   0.004960   0.021002  ...     0.022737    -0.028082   \n",
       "\n",
       "   s_embed_376  s_embed_377  s_embed_378  s_embed_379  s_embed_380  \\\n",
       "0     0.004724     0.026689    -0.033182     0.019103     0.079272   \n",
       "1    -0.019864    -0.008816     0.020599     0.028047     0.087291   \n",
       "2    -0.023322     0.011250     0.010961     0.017847     0.098809   \n",
       "3    -0.004571     0.078000    -0.053182     0.051308     0.046214   \n",
       "4     0.023483     0.006377    -0.031809     0.014866     0.051410   \n",
       "\n",
       "   s_embed_381  s_embed_382  s_embed_383  \n",
       "0     0.126950     0.030770     0.022833  \n",
       "1    -0.032195     0.001828     0.052647  \n",
       "2    -0.034606    -0.001182     0.051882  \n",
       "3     0.022991    -0.108413    -0.007681  \n",
       "4    -0.063010     0.000670     0.030218  \n",
       "\n",
       "[5 rows x 385 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6eed364c-376f-465f-9bca-01ba576c55e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614657, 385)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1441781-e555-48fd-a6c0-3be2630cd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../Data/agg_summary_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ead3151-5268-42d8-a81a-b931561424a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(final_df['embed_0'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5794c9-631c-4682-b80a-811cbf25ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df = pd.read_pickle(\"../Data/agg_summary_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6c478f-559c-4a76-ae7f-dbfe6f7a0fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614657, 385)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506fcf46-d4d9-47f4-b12b-265ab29554f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'embed_0', 'embed_1', 'embed_2', 'embed_3', 'embed_4',\n",
       "       'embed_5', 'embed_6', 'embed_7', 'embed_8',\n",
       "       ...\n",
       "       'embed_374', 'embed_375', 'embed_376', 'embed_377', 'embed_378',\n",
       "       'embed_379', 'embed_380', 'embed_381', 'embed_382', 'embed_383'],\n",
       "      dtype='object', length=385)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4181b8bf-ed5a-4360-b253-172378732920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(final_df['embed_0'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d87056-6d72-4880-90ad-8c9d019fd979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613f926b",
   "metadata": {},
   "source": [
    "# Documentation for `negative-complaint-words.ipynb`\n",
    "\n",
    "This notebook extracts, processes, and classifies words from consumer product incident reports to identify complaint-related words.\n",
    "We use an LLM to classify each token as complaint-related (`1`) or neutral/positive (`0`).\n",
    "\n",
    "\n",
    "## Input Files\n",
    "\n",
    "- `../Data/cpsc_data/incident_reports/Toysandchildren_ArtsandCrafts.csv`\n",
    "- `../Data/cpsc_data/incident_reports/Toysandchildren_Riding_Toys.csv`\n",
    "- `../Data/cpsc_data/incident_reports/Toysandchildren_Toys.csv`\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- `token_labels_individual.pkl` — Pickle file mapping tokens to their complaint/neutral label.\n",
    "- `filtered_word_lists.pkl` — Pickle file containing the final filtered complaint and neutral word lists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent   \n",
    "sys.path.insert(0, str(project_root / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5257000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')       \n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt_tab')  \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to all three files\n",
    "recall_files = [\n",
    "    \"../Data/cpsc_data/incident_reports/Toysandchildren_ArtsandCrafts.csv\",\n",
    "    \"../Data/cpsc_data/incident_reports/Toysandchildren_Riding_Toys.csv\",\n",
    "    \"../Data/cpsc_data/incident_reports/Toysandchildren_Toys.csv\"\n",
    "]\n",
    "\n",
    "recall_dfs = [load_clean_csv(path) for path in recall_files]\n",
    "recalls_df = pd.concat(recall_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f5fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# from the recalls data, embed the incident description\n",
    "combined_indicent_text = \" \".join(recalls_df['Incident Description'].dropna().tolist())\n",
    "incident_desc_embedding = model.encode(combined_indicent_text)\n",
    "incident_desc_embedding = np.array(incident_desc_embedding).reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ecf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the complaints data to remove stop words and get down to lemm\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e0eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "incidents = recalls_df['Incident Description'].dropna().astype(str)\n",
    "all_tokens = incidents.apply(preprocess)\n",
    "\n",
    "# Flatten to single list of tokens\n",
    "flattened_tokens = [token for sublist in all_tokens for token in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc280ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('toy', 2782), ('child', 1810), ('product', 1228), ('old', 1184), ('battery', 1149), ('get', 1129), ('play', 957), ('son', 917), ('one', 908), ('daughter', 893), ('year', 833), ('could', 815), ('come', 748), ('use', 716), ('consumer', 632), ('take', 608), ('small', 601), ('purchase', 595), ('would', 582), ('piece', 580)]\n"
     ]
    }
   ],
   "source": [
    "# Optional: count top words\n",
    "word_freq = Counter(flattened_tokens)\n",
    "top_words = word_freq.most_common(20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba8915-e84a-4c46-8292-4aa62424da5b",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fba0170-2575-4844-ba2f-a97c67e5e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_BATCH_PROMPT = \"\"\"\n",
    "You are an expert at analyzing customer reviews for toys.\n",
    "\n",
    "For each token below, classify it as:\n",
    "\n",
    "- \"1\" → if it frequently appears in reviews describing problems, defects, malfunctions, poor quality, user dissatisfaction, safety concerns, or any kind of negative experience.\n",
    "- \"0\" → if it is typically used in descriptive, generic, or positive contexts and is not strongly related to complaints.\n",
    "\n",
    "Return only the answer as a JSON list of 1s and 0s, without explanation or extra text.\n",
    "Example: [\"1\", \"0\", \"0\", \"1\"]\n",
    "\n",
    "Tokens: {token_list}\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f89860a-c68a-4093-b787-efbef623637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tokens_batch_http(tokens, model=\"mixtral\", host=\"http://localhost:11434\"):\n",
    "    prompt = TOKEN_BATCH_PROMPT.format(token_list=\", \".join(tokens))\n",
    "\n",
    "    r = requests.post(\n",
    "        f\"{host}/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        },\n",
    "        stream=True,\n",
    "        timeout=120,\n",
    "    )\n",
    "\n",
    "    full = \"\"\n",
    "    for line in r.iter_lines():\n",
    "        if line:\n",
    "            piece = json.loads(line.decode())\n",
    "            if \"message\" in piece and \"content\" in piece[\"message\"]:\n",
    "                full += piece[\"message\"][\"content\"]\n",
    "\n",
    "    # --- this is the important part ---\n",
    "    try:\n",
    "        result = json.loads(full.strip())\n",
    "    except Exception:\n",
    "        cleaned = full.strip().replace(\",\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\")\n",
    "        result = [w for w in cleaned.split() if w in {\"0\", \"1\"}]\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "593723e9-1847-4652-8189-dfaa9bb32c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying tokens one by one: 100%|████████████████████████████████████████████| 9149/9149 [11:34:31<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Saved 9149 labels to token_labels_individual.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vocabulary list\n",
    "vocab = list(word_freq.keys())\n",
    "\n",
    "# Output path for labeled tokens\n",
    "OUT_FILE = Path(\"token_labels_individual.pkl\")\n",
    "\n",
    "# Result dictionary\n",
    "labels = {}\n",
    "\n",
    "# Function to classify a single token\n",
    "def classify_token(token, model=\"mixtral\", host=\"http://localhost:11434\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert at analyzing customer reviews for toys and a binary classifier.\n",
    "\n",
    "If the word is commonly associated with **complaints or problems** in toy reviews, respond with **1**.\n",
    "\n",
    "Otherwise, respond with **0**.\n",
    "\n",
    "Respond ONLY with a single digit: 0 or 1. Do NOT include explanation.\n",
    "\n",
    "Word: {token}\n",
    "Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{host}/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        },\n",
    "        stream=True,\n",
    "        timeout=60,\n",
    "    )\n",
    "\n",
    "    full = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            piece = json.loads(line.decode())\n",
    "            if \"message\" in piece and \"content\" in piece[\"message\"]:\n",
    "                full += piece[\"message\"][\"content\"]\n",
    "\n",
    "    # Extract the first 0 or 1\n",
    "    for c in full:\n",
    "        if c in {\"0\", \"1\"}:\n",
    "            return int(c)\n",
    "    return None  # fallback if nothing valid is found\n",
    "\n",
    "# Main loop — classify each token individually\n",
    "for token in tqdm(vocab, desc=\"Classifying tokens one by one\"):\n",
    "    try:\n",
    "        label = classify_token(token)\n",
    "        if label is not None:\n",
    "            labels[token] = label\n",
    "            # Save after 50 labels\n",
    "            if len(labels) % 50 == 0:\n",
    "                OUT_FILE.write_bytes(pickle.dumps(labels))\n",
    "        else:\n",
    "            print(f\"❌ No label found for token: {token}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for token '{token}': {e}\")\n",
    "\n",
    "# Final save\n",
    "OUT_FILE.write_bytes(pickle.dumps(labels))\n",
    "print(f\"\\n✅ Done! Saved {len(labels)} labels to {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45adf29-5077-4a98-874e-74999de68fed",
   "metadata": {},
   "source": [
    "## ChatGPT Eliminated Negative Words and Neutral Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56896a9f-c853-4194-b6f6-80f036a06861",
   "metadata": {},
   "source": [
    "Negative words still need elimination. Use ChatGPT to eliminate them by giving 300 to ChatGPT at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac4a0ea9-8191-4937-ae45-bb6231681ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words labeled as 1 (complaint-related)\n",
    "complaint_words = [word for word, label in labels.items() if label == 1]\n",
    "\n",
    "# Words labeled as 0 (neutral or positive)\n",
    "neutral_words = [word for word, label in labels.items() if label == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a62c7d-a1fa-4e27-860f-997e9e2c2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graduation, nearer, tinnitus, privateer, heath, lingered, squad, scabby, burried, splatr, microhyphema, sandy, sepsis, beating, posed, ignatius, itt, twine, widen, ferrite\n"
     ]
    }
   ],
   "source": [
    "b/home/alex/safeify_video/1 file from Emelie Arvidsson on Jun 26, 2025/Finalversionofintro_export1.movegin_ind = 6150\n",
    "end_ind = begin_ind+300\n",
    "print(', '.join(complaint_words[begin_ind:end_ind]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "1339a15e-ef8f-45af-9b1b-bf3b948b66a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint-related words:\n",
      "['sepsis', 'scabby', 'sandy', 'tinnitus', 'burried', 'splatr', 'beating', 'microhyphema']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of unrelated or neutral words\n",
    "unrelated_words = [\n",
    "    \"graduation\",\n",
    "    \"nearer\",\n",
    "    \"privateer\",\n",
    "    \"heath\",\n",
    "    \"lingered\",\n",
    "    \"squad\",\n",
    "    \"posed\",\n",
    "    \"ignatius\",\n",
    "    \"itt\",\n",
    "    \"twine\",\n",
    "    \"widen\",\n",
    "    \"ferrite\"\n",
    "]\n",
    "\n",
    "\n",
    "# Convert to sets\n",
    "complaint_set = set(complaint_words[begin_ind:end_ind]) \n",
    "unrelated_set = set(unrelated_words)\n",
    "\n",
    "# Get only complaint-related words (i.e., those not in unrelated set)\n",
    "filtered_complaint_words = list(complaint_set - unrelated_set)\n",
    "\n",
    "# Show result\n",
    "print(\"Complaint-related words:\")\n",
    "print(filtered_complaint_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "bfa3c5e8-badd-4b0c-9186-c26ce6da6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of complaint 3044\n",
      "Length of unrelated 3185\n"
     ]
    }
   ],
   "source": [
    "filtered_complaint_words_all.extend(filtered_complaint_words)\n",
    "filtered_unrelated_words_all.extend(unrelated_words)\n",
    "print(f'Length of complaint {len(filtered_complaint_words_all)}')\n",
    "print(f'Length of unrelated {len(filtered_unrelated_words_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3548cdc5-9a01-459a-97ce-cd1d5e7e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both lists to a pickle file\n",
    "with open(\"filtered_word_lists.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"complaint_words_filtered\": filtered_complaint_words_all,\n",
    "        \"neutral_words\": neutral_words\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f07b34a7-d9b9-4eac-b58b-f5ad4b622363",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered_word_lists.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Access the lists\n",
    "complaint_words3 = data[\"complaint_words_filtered\"]\n",
    "neutral_words3 = data[\"neutral_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "488c9b76-cc7d-4cb1-9816-c94bf1fdc225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['complaint_words_filtered', 'neutral_words'])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "cb3f5869-9e23-4702-9d1d-9e89833a227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6164"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neutral_words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7540dadc-b6d4-4e1e-9bad-2abb753dfbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3044"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complaint_words3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576ec59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safeify-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
